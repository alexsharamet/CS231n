{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Чтение файлов\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(W,x):\n",
    "    row=np.asmatrix(x)\n",
    "    ans=W*row.T\n",
    "    return ans\n",
    "\n",
    "def MtrxPrint(M):\n",
    "    n,m = M.shape\n",
    "    for i in range(n):\n",
    "        for j in range(10):\n",
    "            print(M[i,j],' ',end='')\n",
    "        print()\n",
    "        \n",
    "#def SymmetricDifferenceQuote(func,x,h):\n",
    "#    return (func(x+h)-func(x-h))/2*h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RegularisationL2(W):\n",
    "    n,m=W.shape\n",
    "    sm=0\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            sm+=W[i,j] ** 2\n",
    "    return sm\n",
    "\n",
    "def RegularisationL2Grad(W):\n",
    "    ans=W.copy()\n",
    "    ans*=2\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def HingeLoss(W,x,y):\n",
    "    ansF=f(W,x)\n",
    "    sm=0\n",
    "    for i in range(0,10):\n",
    "        if(i!=y):\n",
    "            sm+=max(0,ansF[i]-ansF[y]+1)\n",
    "    return sm\n",
    "\n",
    "def GradienAnaliticHingeLoss(W,x,y,argv1,argv2):\n",
    "    grad=np.zeros((10,3072))\n",
    "    grad=np.asmatrix(grad).astype(float)\n",
    "    ansF=f(W,x)\n",
    "    sm=0\n",
    "    for i in range(10):\n",
    "        if i!=y:\n",
    "            if( (ansF[i]-ansF[y]+1) > 0 ):\n",
    "                sm+=1\n",
    "                grad[i]=x.astype(float)\n",
    "    grad[y]=-x.astype(float)*sm\n",
    "    return grad\n",
    "\n",
    "def BackPropogationHingeLoss(W,x,y,argv1,argv2):\n",
    "    #argv1, argv2 not used, but need to common start different types of gradients\n",
    "    #forward\n",
    "    scores=f(W,x)\n",
    "    margin=np.zeros(10)\n",
    "    reg_loss=0\n",
    "    for i in range(10):\n",
    "        margin[i]=max(0,scores[i]-scores[y]+1)\n",
    "        if(i!=y):\n",
    "            reg_loss+=margin[i]\n",
    "\n",
    "\n",
    "    #backward\n",
    "    #margin with max\n",
    "    dMrgn = np.ones(10)*1\n",
    "    count=9\n",
    "    for i in range(10):\n",
    "        if( (scores[i]-scores[y]+1) <= 0):\n",
    "            dMrgn[i]=0\n",
    "            cnt-=1\n",
    "    #print(dMrgn)\n",
    "    dscores=dMrgn\n",
    "    dscores[y]=-dMrgn[i]*count\n",
    "    \n",
    "    #find dW\n",
    "    dWLocal=np.zeros((10,3072))\n",
    "    dWLocal=np.asmatrix(dWLocal).astype(float)\n",
    "    for i in range(10):\n",
    "        dWLocal[i]=x\n",
    "    dW=dWLocal\n",
    "    d1,d2=dW.shape\n",
    "    for i in range(d1):\n",
    "        dW[i]*=dscores[i]\n",
    "        \n",
    "    return dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SoftMaxLoss(W,x,y):\n",
    "    ansF=f(W,x)\n",
    "    ansF=np.exp(ansF)\n",
    "    sm=np.sum(ansF)\n",
    "    ans=-np.log(ansF[y]/sm)\n",
    "    return ans\n",
    "\n",
    "def GradientAnaliticSoftMaxLoss(W,x,y,argv1,argv2):\n",
    "    grad=np.zeros((10,3072))\n",
    "    grad=np.asmatrix(grad).astype(float)\n",
    "    ansF=f(W,x)\n",
    "    ansF=np.exp(ansF)\n",
    "    sm=np.sum(ansF)\n",
    "    for i in range(10):\n",
    "        row=x.copy().astype(float)\n",
    "        if (i!=y):\n",
    "            row*=float(ansF[i]/sm)\n",
    "        else:\n",
    "            row*=float(ansF[i]/sm-1)\n",
    "        grad[i]=row\n",
    "    return grad\n",
    "\n",
    "def BackPropogationSoftMax(W,x,y,argv1,argv2):\n",
    "    #argv1, argv2 not used, but need to common start different types of gradients\n",
    "    #forward\n",
    "    scores=f(W,x)\n",
    "    expScores=np.exp(scores)\n",
    "    sm=np.sum(expScores)\n",
    "    underLg=expScores[y]/sm;\n",
    "    Loss=-np.log(underLg)\n",
    "    #print(Loss)\n",
    "    \n",
    "    #backward\n",
    "    #log(a/b) - > D(log)*D(a/b) (NOT log(a)-log(b), I understand it in next part.)\n",
    "    #a/b - >  a * 1/b\n",
    "    \n",
    "    #DL/DL=1\n",
    "    Dexp=np.zeros(10)\n",
    "    Ds=np.zeros(10)\n",
    "    \n",
    "    DLogMinus=-1/underLg\n",
    "    #print(DLogMinus)\n",
    "    \n",
    "    DOneDivSum=expScores[y]*DLogMinus\n",
    "    DSum=-DOneDivSum/(sm*sm)\n",
    "    for i in range(10):\n",
    "        Dexp[i]=DSum\n",
    "    Dexp[y]+=DLogMinus/sm\n",
    "\n",
    "    for i in range(10):\n",
    "        Ds[i]=np.exp(scores[i])*Dexp[i];\n",
    "    \n",
    "    #dW\n",
    "    dWLocal=np.zeros((10,3072))\n",
    "    dWLocal=np.asmatrix(dWLocal).astype(float)\n",
    "    for i in range(10):\n",
    "        dWLocal[i]=x\n",
    "    dW=dWLocal\n",
    "    d1,d2=dW.shape\n",
    "    for i in range(d1):\n",
    "        dW[i]*=Ds[i]\n",
    "    return dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GradientNumeric(W,x,y,eps,LossFunction):\n",
    "    n,m = W.shape\n",
    "    grad=np.zeros((10,3072))\n",
    "    grad=np.asmatrix(grad)\n",
    "    lsS=LossFunction(W,x,y)\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            Wt=W.copy()\n",
    "            Wt[i,j]+=eps\n",
    "            ls=LossFunction(Wt,x,y)\n",
    "            grad[i,j]=(ls-lsS)/eps\n",
    "    return grad\n",
    "\n",
    "def Loss(W,X,Y,k,LossFunction):\n",
    "    N = len(Y)\n",
    "    sm=0\n",
    "    for i in range(0,N):\n",
    "        x = X[i,:]\n",
    "        y = Y[i]\n",
    "        sm=sm+LossFunction(W,x,y)\n",
    "    return float(sm/N)+k*RegularisationL2(W)\n",
    "\n",
    "def Gradient(W,X,Y,k,typeFunc,eps,LossFunction):\n",
    "    #common gradient function in type function last two parameters are optional\n",
    "    #I don't cut X_test on chunks to improve speed of learning.\n",
    "    grad=np.zeros((10,3072))\n",
    "    grad=np.asmatrix(grad)\n",
    "    for i in range(len(Y)):\n",
    "        grad+=typeFunc(W,X[i],Y[i],eps,LossFunction)\n",
    "    return grad/len(Y)+k*RegularisationL2Grad(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.30527127]]\n"
     ]
    }
   ],
   "source": [
    "W = np.random.rand(10,3072)\n",
    "W = W*1e-6\n",
    "W = np.asmatrix(W)\n",
    "#test Loss on start\n",
    "#print(HingeLoss(W,X_train[0],y_train[0]))     #n-1 (n=10) \n",
    "#print(SoftMaxLoss(W,X_train[0],y_train[0]))   #-ln(1/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Compare gradients\n",
    "#MtrxPrint(BackPropogationHingeLoss(W,X_train[0],y_train[0],0,0))\n",
    "#print()\n",
    "#MtrxPrint(GradientNumeric(W,X_train[0],y_train[0],1e-5,HingeLoss))\n",
    "#print()\n",
    "#MtrxPrint(GradienAnaliticHingeLoss(W,X_train[0],y_train[0],0,0))\n",
    "#print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#MtrxPrint(BackPropogationSoftMax(W,X_train[0],y_train[0],1e-5,SoftMaxLoss))\n",
    "#print()\n",
    "#MtrxPrint(GradientNumeric(W,X_train[0],y_train[0],1e-5,SoftMaxLoss))\n",
    "#print()\n",
    "#MtrxPrint(GradientAnaliticSoftMaxLoss(W,X_train[0],y_train[0],0,0))\n",
    "#print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(W,X,Y,k,LossFunc,GradientLoss):\n",
    "    itr=0\n",
    "    change=999999999\n",
    "    epsMethod=1e-5\n",
    "    step_size=1e-5\n",
    "    before=Loss(W,X,Y,k,LossFunc)\n",
    "    while (change > epsMethod and itr<1000):\n",
    "        W-=step_size*Gradient(W,X,Y,k,GradientLoss,0,0)\n",
    "        after=Loss(W,X,Y,k,LossFunc)\n",
    "        change=abs(before-after)\n",
    "        before=after\n",
    "        itr+=1\n",
    "    return W\n",
    "\n",
    "def test(W,x):\n",
    "    ans=f(W,x)\n",
    "    mx=-9999999\n",
    "    maxPos=-1\n",
    "    for i in range(len(ans)):\n",
    "        if ans[i]>mx:\n",
    "            mx=ans[i]\n",
    "            maxPos=i\n",
    "    return maxPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read data from file\n",
    "dataSet = unpickle(\"data_batch_1\")\n",
    "labels=dataSet[b'labels']\n",
    "data=dataSet[b'data']\n",
    "filenames=dataSet[b'filenames']\n",
    "MaxSize=1000 #10000 max\n",
    "labels=labels[:MaxSize]\n",
    "data=data[:MaxSize]\n",
    "filenames=filenames[:MaxSize]\n",
    "\n",
    "#Train, Validate, test split\n",
    "X_all, X_test, y_all, y_test = train_test_split(data,labels,test_size=0.2,random_state=0)\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_all,y_all,test_size=0.2,random_state=0)\n",
    "\n",
    "#Create matrix W\n",
    "W = np.random.rand(10,3072)\n",
    "W = W*1e-6\n",
    "W = np.asmatrix(W)\n",
    "LossFuncArr=(SoftMaxLoss,HingeLoss)\n",
    "GradientFuncArr=(GradientAnaliticSoftMaxLoss,GradienAnaliticHingeLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function SoftMaxLoss at 0x112ab5d90>\n",
      "k = 0.01    0.25625\n",
      "k = 0.1    0.32500\n",
      "k = 0.5    0.32500\n",
      "k = 0.9    0.31875\n",
      "k = 1    0.31875\n",
      "\n",
      "<function HingeLoss at 0x10b03fa60>\n",
      "k = 0.01    0.31875\n",
      "k = 0.1    0.31875\n",
      "k = 0.5    0.31875\n",
      "k = 0.9    0.31875\n",
      "k = 1    0.31875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for L in range(2):\n",
    "    LossFunc=LossFuncArr[L]\n",
    "    GradientFunc=GradientFuncArr[L]\n",
    "    print(LossFunc)\n",
    "    for k in [0.01, 0.1, 0.5, 0.9, 1]:\n",
    "        W=train(W,X_train,y_train,k,LossFunc,GradientFunc)\n",
    "        good=0\n",
    "        for i in range(len(X_validate)):\n",
    "            ans=test(W,X_validate[i])\n",
    "            if(ans == y_validate[i]):\n",
    "                good+=1\n",
    "        print(\"k =\",k,\"%10.5f\"% (good/len(y_validate)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-9756c6fca07e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mW\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSoftMaxLoss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mGradientAnaliticSoftMaxLoss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "k=0.01\n",
    "W=train(W,X_train,y_train,k,SoftMaxLoss,GradientAnaliticSoftMaxLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-4d1c95ea8c3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgood\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mgood\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "good=0\n",
    "for i in range(len(X_test)):\n",
    "    ans=test(W,X_test[i])\n",
    "    if(ans == y_test[i]):\n",
    "        good+=1\n",
    "print(\"%10.5f\"% (good/len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
